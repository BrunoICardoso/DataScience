{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e7b9642c20d4571d62013fbbfbcb8849dc512afe"
   },
   "source": [
    "# Trabalho 3 - Machine Learning II - Deep Learning\n",
    "Prof: Carlos Padilha\n",
    "\n",
    "#### Alunos:  \n",
    "\n",
    "Roberto A. Coutinho  \n",
    "Thais Galho\n",
    "\n",
    "**Enunciado**\n",
    "\n",
    "1.  Introdução a Deep Learning\n",
    "    Este trabalho visa avaliar o entendimento em relação as redes convolutivas e recorrentes vistas em sala para resolver diversos problemas.  \n",
    "    \n",
    "    \n",
    "2.  A proposta é que a dupla escolha alguma aplicação que mais se interessa e resolva usando os conceitos vistos em sala. Segue alguns exemplos de problemas que vocês podem escolher:\n",
    "\n",
    "    * Classificação de imagens\n",
    "    * Geração automática de imagens • Reconstrução de imagens\n",
    "    * Análise de sentimento\n",
    "    * Sumarização de textos\n",
    "    * Predição em séries temporais\n",
    "\n",
    "Obs. Importante lembrar que é desejado que vocês escolham bases de dados diferentes daquelas usadas em sala.\n",
    "O trabalho pode ser feito em dupla e deve ser enviado por email (carlos.engcomp@gmail.com).\n",
    "\n",
    "**Descrição do trabalho**\n",
    "\n",
    "    1. Decidimos utilizar redes convolutivas para classificação multi-class de imagens de flores.\n",
    "    2. Como otimizador usamos o Gradiente Descente Estocrático\n",
    "    3. Flores: Tulipas, Dentes de Leão, Rosas, Girassóis e Margaridas\n",
    "    4. Usamos o Dataset e ambiente de treinamento no Kaggle.\n",
    "        * GPU on\n",
    "        * Tensorflow + Keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c8f1cff4f367b7eeccc4b4b7cbd80fbae5331c15"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-971fd550adb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Importing the Keras libraries and packages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from PIL import Image\n",
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from tensorflow.contrib.keras.api.keras.callbacks import Callback\n",
    "from tensorflow.contrib.keras.api.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.contrib.keras import backend\n",
    "\n",
    "from keras.optimizers import *\n",
    "from keras.metrics import *\n",
    "from keras.callbacks import *\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "script_dir = os.path.dirname(\".\")\n",
    "training_set_path = os.path.join(script_dir, '../input/flowers/flowers/')\n",
    "test_set_path = os.path.join(script_dir, '../input/flowers/flowers/')\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura dos datasets de treinamento e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aa8990ba88e65994e4429abc8eebedd8a8679d41"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "input_size = (256, 256)\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255, \n",
    "                                  validation_split=0.33)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(training_set_path,\n",
    "                                                 target_size=input_size,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 subset=\"training\",\n",
    "                                                 class_mode='categorical')\n",
    "\n",
    "\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(test_set_path,\n",
    "                                            target_size=input_size,\n",
    "                                            batch_size=batch_size,\n",
    "                                            subset=\"validation\",\n",
    "                                            class_mode='categorical',\n",
    "                                            shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definição das camadas do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dd999bedc98123b35be68bbeb8645f2d9fbc2f52"
   },
   "outputs": [],
   "source": [
    "classifier = Sequential()\n",
    "\n",
    "classifier.add(Conv2D(32, \n",
    "                      (3, 3), \n",
    "                      input_shape=(256,256,3), \n",
    "                      activation='relu'))\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2), \n",
    "                            data_format=\"channels_first\"))\n",
    "\n",
    "#Second\n",
    "classifier.add(Conv2D(32, \n",
    "                      (3, 3), \n",
    "                      activation='relu'))\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#Third\n",
    "classifier.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Full connection\n",
    "\n",
    "classifier.add(Dense(units=64, activation='relu'))\n",
    "classifier.add(Dropout(0.5))\n",
    "classifier.add(Dense(units=5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definição da função de callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "02b13e467e5921a84bafaafdb50c7f0b0025fe53"
   },
   "outputs": [],
   "source": [
    "class LossHistory(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.epoch_id = 0\n",
    "        self.losses = ''\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.losses += \"Epoch {}: accuracy -> {:.4f}, val_accuracy -> {:.4f}\\n\"\\\n",
    "            .format(str(self.epoch_id), logs.get('acc'), logs.get('val_acc'))\n",
    "        self.epoch_id += 1\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses += 'Iniciando treinamento...\\n'\n",
    "        \n",
    "history = LossHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilação do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e8e816bcd4d7fade037b3e33994a91f57c4ec3fb"
   },
   "outputs": [],
   "source": [
    "#opt = Adam(lr=1e-3, decay=1e-6)\n",
    "opt = SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "\n",
    "classifier.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execução do treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "84cd4d6db6d50be1885d22226a2c30c448ef08cb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch=1000/batch_size,\n",
    "                         epochs=90,\n",
    "                         validation_data=test_set,\n",
    "                         validation_steps=100/batch_size,\n",
    "                         workers=24,\n",
    "                         callbacks=[history])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4132e79a792eb31c17bd04b95aa4a5b18cf946c9"
   },
   "source": [
    "# Apresentação dos Resultados\n",
    "\n",
    "  \n",
    "  \n",
    "Nessa seção são apresentados os resultados da execução do algoritmo AdaBooting, bem como observações gerais feitas ao longo do processo de desenvolvimento, experimentos e simulações.\n",
    "A primeira lista são os resultados com a base de treinamento e a segunda lista resultados considerando a base de testes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "c82176ffd61313469ff1f9ba4567ab46c91510c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7678724118832791, 0.7212078651685393]\n"
     ]
    }
   ],
   "source": [
    "score = classifier.evaluate_generator(test_set, 45)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "75dbf4220d223745ed292c8a685ee2b2c49f7237"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1424 images belonging to 5 classes.\n",
      "Confusion Matrix\n",
      "[[178  43  13  10   9]\n",
      " [ 10 297   5  30   5]\n",
      " [ 11  31 107   6 103]\n",
      " [  8  18   1 207   8]\n",
      " [ 11  28  26  21 238]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "validation_set = test_datagen.flow_from_directory(test_set_path,\n",
    "                                            target_size=input_size,\n",
    "                                            batch_size=batch_size,\n",
    "                                            subset=\"validation\",\n",
    "                                            class_mode='categorical',\n",
    "                                            shuffle=False)\n",
    "\n",
    "\n",
    "#Confution Matrix and Classification Report\n",
    "Y_pred = classifier.predict_generator(validation_set, 45)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(validation_set.classes,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "ac85c23c1956ba1b5d7e39292c736af01436bbc3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7212078651685393"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(validation_set.classes, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "28e17ec51bf582d086f668b63d3b25a66d463a3a"
   },
   "source": [
    "# Conclusões Finais\n",
    "\n",
    "Observando através de diversas simulações, podemos ver pelo comportamento do erro de aprendizagem dos modelos na medida que vai sendo minimizado o resultado da acurácia, precisão e recall tende a ambos se equilibrarem de forma a maximizar o desempenho entre eles. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "487015bcb9bbe67b49e8898addb1ffa1bf5b4c0e"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
